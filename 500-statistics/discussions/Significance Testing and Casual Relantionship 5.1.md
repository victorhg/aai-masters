
# What to do
Respond to the following prompt by Day 4 of the learning week. Cite resources and references appropriately in APA format.

Discuss some of the pitfalls of statistical significance testing and how it can be misinterpreted and misused. Then, discuss ways this approach can be improved, especially when attempting to establish causal links in studies.

# answer

[[@filippiniRoleStatisticalSignificance2022]] Present how statistical significance/null hypothesis testing is being increasingly criticized and abandoned in the reporting and interpretation of the results of biomedical research, and showing evidence of its misused by multiple actors in corporate and policy making. But the problem of using statistical significance as the main measure of causality is a serious concern within the scientific community.

Described as a widespread problem [@headExtentConsequencesPHacking2015], presents the practice of researchers collecting data or statistical analyses until nonsignificant results become significant. Focusing solely on statistical significance without considering effect size can mislead researchers about the practical importance of their findings. A statistically significant result may have a trivial effect in real-world applications. 

This practice is also called P-Hacking, and such behavior is said to be a major contributor to the large number of false and non-reproducible discoveries found in academic journals [@StatisticalCrisisScience], classified as the Replication Crisis.

[@fitzpatrickImpactRedefiningStatistical2024]  propose alternative approaches to mitigate this problem using methods such as Bayesian statistics or confidence intervals that provide more nuanced insights into data without solely relying on arbitrary thresholds like p < 0.05 an d Encouraging Transparency: Shifting away from strict adherence to statistical significance could promote more transparent research practices, like open data sharing that reduces the temptation for p-hacking.

Decision-making based solely on p-values for making critical decisions without considering the broader context or additional evidence can lead to poor outcomes. Simplistic interpretations of statistical significance can distort public understanding and policy decisions based on research findings.

Reconsidering the reliance only on statistical significance testing can lead a more accurate representation of findings to better inform population and guide decision making.

# Prompt





P-hacking is widespread occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant [@headExtentConsequencesPHacking2015]

In assessing the safety risk posed by a product, the FDA considers factors such as ‘strength of the association,’ ‘temporal relationship of product use and the event,’ ‘consistency of findings across available data sources,’ ‘evidence of a dose-response for the effect,’ ‘biologic plausibility,’ ‘seriousness of the event relative to the disease being treated,’ ‘potential to mitigate the risk in the population,’ ‘feasibility of further study using observational or controlled clinical study designs,’ and ‘degree of benefit the product provides, including availability of other therapies’”

A Case Against Statistical Significance Testing
Given the issues surrounding p-hacking and neglecting effect size, there is a growing argument for moving away from traditional statistical significance testing:
Reproducibility Crisis: The replication crisis highlights how many published findings cannot be reproduced, often due to p-hacking and misinterpretation of significance tests. This calls into question the validity of relying on p-values as definitive proof of an effect 12.
Alternative Approaches: Researchers advocate for methods such as Bayesian statistics or confidence intervals that provide more nuanced insights into data without solely relying on arbitrary thresholds like p < 0.05. These methods allow for a more comprehensive understanding of data variability and effect magnitude 34.
Encouraging Transparency: Shifting away from strict adherence to statistical significance could promote more transparent research practices, encouraging preregistration and open data sharing that reduce the temptation for p-hacking 5.
In conclusion, addressing p-hacking and emphasizing effect size are essential steps toward improving the credibility and utility of scientific research. By reconsidering the reliance on statistical significance testing, researchers can foster a more accurate representation of findings that better informs scientific discourse and practical applications.

# References

FILIPPINI, T., & VINCETI, S. R. (2022). The role of statistical significance testing in public law and health risk assessment. Journal of Preventive Medicine and Hygiene, 63(1), E161–E165. https://doi.org/10.15167/2421-4248/jpmh2022.63.1.2394 Links to an external site.
Fitzpatrick, B. G., Gorman, D. M., & Trombatore, C. (2024). Impact of redefining statistical significance on P-hacking and false positive rates: An agent-based model. PLOS ONE, 19(5), e0303262. https://doi.org/10.1371/journal.pone.0303262 Links to an external site.
Head, M. L., Holman, L., Lanfear, R., Kahn, A. T., & Jennions, M. D. (2015). The Extent and Consequences of P-Hacking in Science. PLoS Biology, 13(3), e1002106. https://doi.org/10.1371/journal.pbio.1002106 Links to an external site.
The Statistical Crisis in Science | American Scientist. (n.d.). Retrieved September 30, 2024, from https://www.americanscientist.org/article/the-statistical-crisis-in-science Links to an external site.
